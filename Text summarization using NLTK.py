#!/usr/bin/env python
# coding: utf-8

# In[9]:



import nltk 
from nltk.corpus import stopwords 
from nltk.tokenize import word_tokenize, sent_tokenize 



# Input text - to summarize  
text = """Hello i am i am done. Tu hai jo shiva. Sorry i missed you today. Tanvi shreya barat. I am in. I think udit chaitra is not there maybe she is going out somewhere out. I just spoke to chitra she is actually not well. By mistake. Amin injured her hand and she has to go to a doctor. So she won't be able to join us for today's meeting. Alright. Find the latest continue with josh was part. Ok. Sojat recap. So what you done so far funny you you came up with the introduction of then the justification why have you chosen the multiplicative inverse of new york taxi fare. And moon. Simple picture of karakoram pass. Sofia ramdev. Now i will be adding my path to the source. Film dil aatank shares also understand that right thing. Yesterday's i have made a plan for doing data analysis and also joshua. Can you be a little more louder because i'm your you clearly. Ok now. Yo yo voice is audible and guys one more thing before like when when we will be editing aap 800 letters also device that in practice it so that you we are very well tune and with what we have to say. Jab tak. Golden social site with my production. So people are my part of kafka consumer. Aapka consumer is basically the extension of kafka producer and producer and consumer comes under one single block. Photo caption extension from production park. What does kafka consumer 200 basically as we all know. 2 month producers sends messages to kafka those messages are stored and made available to the consumers through the subscriptions to topic so what does the subscription to the topic mean to basically the messengers whatever. Poland from producer chup made into specific topics for just like how we subscribe to specific channels to keep receiving the 11th of the messenger get subscribe subscribe to the relevant topic and includes creating the messages that saudi producer and consumer and also consumer can read messages starting from the specific offsets are basically the topic. And also it allows the reader to given to the offset point which are the issues and. Consumer to join the classes at any point during the during a cycle any point of point in time during with the cycles. The product of the producer and consumer cycle. Basically any question sofa. Just one question 200 can you please describe something about cough ka little bit. Ok so you like. Kakka kakka. Technical thing about pr train to derive the messengers tomorrow message broking sub broker broking intraday brokerage company operates like you are you your partitioning all you topics on hollywood feeds into relevant 1111 subscription triangles and everything and the new train to derive your messages from the special from the specific and designated topics so that i will at more when i take you guys for world with the connecting point. I can help you with that. Short. Musically. The topic is considered one topical shed the name or category with the messages will be published so once that is established that each topic of for the subdivided into partitions like you remember what barat spoke about the partitioning when you are introducing us to the. Kafka producer thing right social the extension of the partition to wire using partition this basically the consumer. Who is actually helping others it is trading as to to derive the messages from such partition and the partition split the data across different notes. Bhai yah kab ka broke again so when you ask me what is kafka share it comes to kab ka is actually a brokerage system where to place the data according to the different notes and then we consume what took it from the relevant and suitable for the actual for the apple of the eye. Article related to data. And in each of the messages within the partition there will also be an id number which will be assigned to the messages. So via via via via training and id number i will tell you that so i also have some screenshots for which shows and anatomy of kafka partition so i'll just take any help us to understand how does it look so very simple imagine you have for partition for the partition always starts from 0. Partition 0.11 partition to one partition three imagine you have 12 messages in partition 0. Do you like to wish felicity heartfelt different blocks. Blocks are basically the shelf different cells which means each cell consists of set own designating the sage in the cube. You have a nine clothes and partition 1947 partition 2 and 8 blocks and partition 3 respectively. And one thing one more thing or something interesting to add your so here are the messages which will be just like the last one will be the first to know if you're having a block so that has hit the 12th block will be delayed first will be taken out was not the one with filling the cell 0. The basic lessons of life. Angry questions over. Naushad clear for me. Ok ok. A representation of women for better understanding i have a faint 3. 302b to the boxer with. 9 9 9 2 9 9 9 9 9 9 4. Pokemon sun on island to spiders 92 93 94. Ok so now coming back to talk to the brokerage message system now. I know this is basically then i will brokerage for some write for now we should also have some local id with very important. We will never be able to pull out the respective message from the respect to broker made a broker id is equal to 1. And 92 will be the leaders of 992 will be the leader 99394 with the follower now i'll tell you want to see a leader and follower. Dholida basically the first one and the old one and the following are basically the the the backup of 90 to remember when speaking about having multiple partitions and in what is the use of having multiple partitions basically. Backup messages something goes wrong with the first. Something goes wrong with the leader the followers will be able to backup the leader. So that the council between the leadership of a good leader and follower. It is not that class or that we spoke about the oscar producer. Holi ke pass. Bacchan tera few designated packages which we can't miss. Wellington st villa to input kakkad and famous after and import node from taking because we are using structure and milati input kakkar train from the pie custom and will not also important because we are dealing with a message if they arrived so we're dealing with the messages on the data will be sent as messages so the messages will be an audit date extension. From which state is capital. I can relate the cold and cough can see what is the country to send the country to message from broken. And deploy physical using pykafka library for the message system. We like to be able to declare the local holes so you number 92 93 94 192 being a leader and when to take the leadership at the cold so i am going to take care 93 localhost. Localhost. Bam. The basic configuration something which is. Interesting about the interested with the time of fame i didn't give same one doesn't have you think you'll also need specific time of particular time for you. So that could be configured 2009 second. Vitamin b12 message treatment with the regular interval audio the commitment level could go up to 5000 seconds but i made it. Thousand milliseconds because we're not going to handle anything month. Going to be completely different or in a bipolar. Yah message after message. Hot i will the early hearing the code for phone once you start working on the paper craft of the presentation draught. From my. Consumer court. Hello. Really good. Thank you sure. Thank you so it was quite detailed. Agree with this is very detailed. Would you mind if i begin with my analysis part now. I think you should go with partner. But describes itself i am going to mute mic so that we don't get any it turned out from a room. So don't mind me if i don't respond if they ask in a text android. Nobody. Koi. So let's start with the data analysis part of mine. So in order to gather some insights about the data so i have a plan regarding the following techniques that we can apply in python. So number one would be the libraries that we can use pandas numpy mat plot seaborn datetime. Titan we can import all these libraries. And then we can load the data set in pandas dataframe and then converted into a format call json format. Then we can grab some information about the data set like work on the columns the type of data types. How many rows. And columns the dataset is having. And then regarding the analysis we can two types of analysis 1 is univariate analysis and bivariate analysis. Univariate analysis means that we can use only one variable and bivariate is we can take two variables and see how is the relation between them. So talking about univariate analysis we can check that how many passengers are travelling together. So we observe that a. One passenger mostly avails the cab services and a higher number of passengers using it together they are very less. And then we can find what is the range of travelling distance in new york city for most of the passengers. I informed that most of the passengers travel 024 miles. Aunty pal with trip distance as zero mile have similar pickup and drop location. Due to last minute cancellation of the cab or being an outlier lawyer it could be excluded i mean. And talking about the bivariate analysis where we can compare. Two variables all together. To the longer trip distances are taken by 12 passengers and no longer trip distance is a covered by higher passengers counts like 5 or 6. Under. Another analysis that we can do with how is are congestion charge imposed on passengers. So it was found that the relationship between trip distance and congestion surcharge is nonlinear. It is observed that small and distances also charged the extra fare which might be due to traffic congestion during a right. Then we can find who is the most preferred vendor so there are two types of vendor id as per the data set representing the two taxi companies. The vendor 1 provides mostly short trip distances cab facility while wonder to provide both short and long. Now we can also do a regression with this model and form a linear regression model for a new york taxi fare amount. Which is the dependent variable and trip distance as an independent variable and then we can check how. Earth. Share is dependent on the distance. And likewise we can do this. Linear regression. In a model. By importing. Linear model from escalon library and so on. And then we can also see there is a linear relationship captured between distance and fare charge. As your distance is increasing the fare is also increasing. After splitting a training and testing data with try to fit the model in the best fit regression line. As per the slope determination. I think the fair amount increases is by. The unit of two for every one unit increase in distance. Swim the following steps. We notice that the future pay amount is credited. And can be detected based on distance covered. And handsome it could be compared with the testing data in order to ensure the best fit model accuracy. Last but not the least we can also do a nice visualisation of a data set. So we can form an interactive map. Using the default basemap with the help of a package cal folium package. And eyes i must tell you this is a very interesting package that i came across. With my research so folium is the name of the package and you can actually have a very nice map visualisation. Where you can display all the pickup cab locations with the help of pick up latitude and longitude. Data point. And then with the help of this sort of data we can actually plot all the locations of a pickup and drop. I think this would be a very this would be a brownie point for a data visualisation. So i think that's all ask for my research. Anaya ko me. We are able to go as per the plan and finish it on time. What do you think about it. Gujarat st bus time. Yes shreya thank you so much it was given to me also guys regarding the storage path. A structure is responsible for the storage path i hope she is able to make some progress. So maybe in the next meeting we can. Take her inside and we can probably talk about her progress. And if you have any additions. Ok changes please reach out to me. We can change it. But make sure it is not a last minute change. Should be prior to the last date. Ok i think you are good to go now and that's all for today. I also think so. Do you have anything to add photo to the gender. Actually i was on mutant i was talking current realise that i was on your ok so as a nice work done it was very informative i think we're still now i think everyone has done their formal letter is left. So we will wait for her in next meet properly. Yeah i am thank you guys for putting a lot of effort into this. And i truly appreciate whatever you have come up with. And i hope we can deliver it on time. We can deliver it efficiently. Char dham. So let's catch up next week when it is ready. Ok have a nice evening everybody. Bye. Mobile. Sia.Hello guys how is your weekend. Totally my way convert code. Mine was good so whose else is on the call. Tu hello hello charitra. Hybrid. Out of tom so i am already finished my path. And looking forward for chitra spot. And. Are you prepared with your path. Can you take us through the small. Dora and share. I think there are busy in some other projects because i spoke to shreya this night. And did a joshua to told something about him where is he right now what he is busy with. Sorry guys i've been there to the last a call i will come with another project. Tried i made significant progress with my storage path in mongodb. So i just take you guys to that. Storage storage path in the medium that i would recommend it as the data stored as documents. And come to the mongo db to. About work and flexible. Make and support a wide variety of data. And considering on your data set to navoday we would be known. The script. Working on. I am in a park completed so i was actually going through the libraries and then at step 1 and pandas iit of these two library. And then after that i came with. Aaj not connecting the connecting the db and moni client. That i was researching on. Affecting a packages like of coc link. State parameter. Cannot provide a local host and the in the coronavirus 9092. And then i also went to the market call climb the topic. Parameter of random topics. And that actually provide. Mechanism to connect to the client. And in the country i made. Some research on how to connect to the database. And that. I can a package call mongodb pymongo dot a mongoclient which takes in the parameters in url link after every day and then we connect to connect to the database. We have to. Ensure that we get access to the collections so in that. To provide. Collection name to the. Reshma devi. Anda news. How to make election part and. Best i. You know communicating with the consumer and store it in that i came up with the topic get simple and i put the simple for loop. Aur. To load the document one by one. I am research and i came up with. Hello how do you think. Aap it is a very a systematic step thanks for doing it because i think you are very busy with the before the weeks when we all did our path and yes it is a very good work that uses explain but i just have few questions is it ok if i if i question my doubts to do what was the advantage of using this pymongo package. Pymongo package actually link your. Real python. So the framework in our process is basically pulses and storage which we are using is. Open link that we're using the package cal pymongo this is the only you know packages available which is supported in python using pymongo rather than any other database like. My secular in anything. Ok ok thanks for answering. And i don't have any more doubt but it was great to hear your part because now it is clear to me whatever you explained. Bharat do you have any questions. Yeah i have one question chitra like one after we store the data. Can i know in what format it stores in mango db server. I think it is stored in the documents format. And then i need to research on this. I didn't go to the step well i retrieve the data from mongodb. But when i talk you. Comic. Thank u 17 data. 8.51. But you should reply but i am back to you and get back to you. Expression. Thank you so much your work was really create a which raste sir good continuation after the church was work. Typing income. Shreya. Set an alarm at. Yes mostly she did the analysis part so a meeting chitra we can ask her next week because then we are having her next upcoming step of the project right so now we have done all initial parts and let's see what next step we have to discuss when all other group members are present. Yah akshay i went to get harm near data sets some interesting civilizations that i am a person who is taking the visualisation of conforming to in next week when all other group members are present we can just have this discussion about the same. Ok guys alright it was nice collect connecting with you guys so let's make next week now. Thank you everyone bye-bye.Hello eicher truck could you please tell me about your friends. Ancestor from india currently pursuing masters in big data and business analytics. Ramesh harish university. Play out this i had 1.8 years of experience in insurance as a data analyst. Ashok myclient hyperion insurance group. Recognise and could you speak about your roles and responsibility. Ok i hai param data quality and management team was responsible for data extraction and reporting for various hyperion teams worldwide. I was also responsible for automating various daily and monthly reports which health different things reduce the manual effort. I practically took part in a stakeholder meetings by which we gain exposure to entrance ad hoc process. Interesting and chat whats your academic background. I am completed my bachelors in computer science and engineering in india and currently in my masters. I gained exposure to various machine learning algorithms and state-of-the-art technologies used in these lakes data engineering and data management. Extra. Tatti unko what you have correctly. Ok please i use r studio for automating tasks. An excel for reporting and i also you know used cat modelling which is a domain specific machine learning algorithm for solving various business question. And i am currently learning python which is part of my academic so. Kill my cousin you know data science. Thank you. Use of you thank you thank you thank you. Thank you.Hello guys how are you doing. Hello tanvi humko how are you. I am also good. So i think today bharat is going to share his part because that was decided earlier right. Ab to tumse. Yes yes yes. I hope has everyone join the meeting. We are there hi prayer. Hi guys how how have you been. Yeah we are good. And what about you. 2 i am just prepared for the data transformation process. So i will just prepared the initial step for sending process. And then how have you been. It's been completed. No no i am talking about how are you. Superhit how have you been. I am good. Leke quite busy like preparing for. Best of which we are going to discuss on after de succession extraction and data transformation process. Yeah but it's been completed like i think i will go ahead with the forces of which i am going to explain. Tracking. We are looking forward to hear from you. That's great. Yeah. Socialist starts from. So as we discussed in our last session. New york taxi data set. As we all know. To the first process out of that is like no extraction. So we need a data set that has to be extracted from. That has to be extracted from the official website or to get api in our case of economy or taxi at weather flexible way. So we can directly download the data third from their official website. To be directly downloaded and just store it as a csv file format. Next step would you like to send the data set we are apache kafka. Process is just an extraction transformation and then we are loading into the storage but. Explained. Why my team mate. Go to start of it. The first process what we are doing this abhiyan setting up environment for transformation so that can be done using python like we're just integrating customer and python. We have an excellent of package cal pi cup cup which does all the transformation process starting from producer to consumer. That was the first death to as soon as be. Extract the data from the official website. We are creating a producer api with stress at thirsting medium between the producer and as well as the extracted part. So once it has been said that make your first job is to like a no. We have to set up multiple broker lekin of multiple broker as multiple server. Because while they are transferring data from kafka producer to the kafka consumer that is the constructed or broker in between which states as a medium where i would like to explain the complete process starting from kafka producer. Prerequisites to be done. So the first page of website before sending the data is to be like you know i had to. Start how much brokers are using so in our case we are using three brokers. Vivo case we are using because like our. In rakesh one having lost click all the data will be. Stored on other broker. That is the reason we are using three brokers. Chotu start off with like a i am just. So it is just a basic process to start the server using command prompt and we have to create a broker id as we all know 0-1 and we have to mention the local port address. 2 and a path so these are the ways to start off with. Brokers. To starting the zookeeper which act as a managing device. You know to start the transformation process i have a doubt in here sorry to interrupt can i ask for doubt. You can what is zookeeper can you please tell us. Zookeeper actually manages all your data for example if you are storing your data. You are sending the data from kafka producer to the sabka country. All the best of luck you know whatever. Process. For example your data will be stored on sabka broker that we all know one captivate confirmed from kafka producer. Autocop car consumer so this all contains the data what we have to do keeper maintenance all these comments this transfer process. To all the commands which is being carried out by kafka producer for cluster after broker and kafka consumer so all the commands which we are carrying out an all the configurations which we are doing it out. It maintains everything so that maintenance job is done by zookeeper. Ok thank you. Yeah so. Tourism paid ceo in orchid resort over the overall scenario will be as i said you that we are using a single producer with three brokers. To any type of multiple partition. Multiplication factors of 3. To start off with a just using python to incorporate all the methods what we have what i have said so far. So we are acid you like we are. We are using a packet ka so from backup karva using kafka producer. Mukesh and we are using pandas package of asleep. And we are using a json package because the format of a data should be in a json format only then we can store it. To return mongo db atlas. Show the first of what you doing this so once you have imported all the required libraries we are just reading the file. Ab tod like on the local system and we have to mention or type as string. So once we are imported the top rated 5 so it has to be converted into a json format want you are converted it we are just loading the file into kafka producer by using the command cal kab ka producer and to make it exactly so accurate so flexible extremely efficient we are using many configuration. It was one of the. Good park in our project. Obviously if you see some of the configuration if you want if i want to mention that we are using like x configuration retries max in flight request for connection retry back of buffer memory to this configuration is like for example if we want to explain any one of those. Apps configuration like it gives you the acknowledgment whether your data has been sent from kafka producer to the cluster acknowledgement of whether it has been sent or not. Sohi. Retry. Configuration. Review. 5 times it will automatically retry. Your. Data again and again. That was that. And you are mentioning the buffer memory size. Like. Your data sizes leke so that let you know. It doesn't cause any. Overflow. So compression type is where is. Compressor sar. Entire data so that like. Transport very fast. And you can decompose it once it has been. Consumer. So obviously we are the mentioning backside. Like a we will be sending out data sent to many batches. How much. Which batch. Contain the pure mention. So and we have a lot more to. Special things about the configuration. Producer. Idea using the encoding obviously never the 10 code is like utf8. What is the standard in coding. Security purpose. Like all what i have. Producer process and along with the land on. Glycol. I am just created to. If at all if our data has been completely still. We will get rs. For example. If all the data transfer. If the data has been. Received. On the. Akka cluster or kakka broker and by all three brokers. We will get an acknowledge. Acknowledgement is nothing but. We will get a. Topic. Partition how many partition we use. And rocks. We will be getting these three. Response. Call. If our data has not been. As for the commando. Function what i have created it. Review. It will be mentioned as i am error back. This is just an additional acknowledgement. Security purpose which are just made. Finally we are. Flashing it with a. Method which. Available on the producer function. So this is all about my part. I have made. Extremely efficient accurate as well as of. If i told you want to handle with the large data transfer process. Configuration spot bihar news. Explain the efficient enough. That was the special thing which we can. Which will be the highlight. Related thing on our. Epds ap. How is the guys like a what you feel is any feedback or is there any. Questions you want to throw up. What your work. Find the story villa 2. Location to speak on this technique elapid behind the above so you'll have to tell them what does a project or what are we trying to do with the data set. Open. Subah really impressive. Taking the time duration into consideration for vaguely related to just turn others apply the concepts into a problem for project. Something about. But find that will keep going up. You don't change anything for now this i want what you down but it turns out any way we can we can come. That perfect us why agree to the point of i try to explain all the original stuff what i have gather. Are just for the purpose of explanation but when we are presenting br2. Modify this i completely agree to that point definitely because on this time train a bi won't be able to explain research process. I have nothing to. Coming. Just a little suggestion bharat it was really great work with you did it was so informative that we are quite clear and also i can i ask you that part which i was not clear on just one suggestion i would say if if the text are repetitive you can edit it because it can save time and likewise it will be useful as well. What do you think. Dekhne ke liye all the duplicate text or explain the path liquor i will be removing all those things to make it a much better. Efficient enough. We have to include all the important step what we think is much relevant to the kaksha produced part. Do i just need your help. Leica water to be modified and how it has to be modified. Definitely we can make that part. Sorry but a very very detailed and very nice work done thank you bharat. Thank you. Ok guys to. Ok guys let's meet next week with i think josh you are shreya. Tummy but i have one suggestion i am almost done with it ok if it is true that. Yeah yeah sure so you want to take a time of let se to 1 to 2 days or you just have to start immediately your 100% ready with it. Cosmetics on the on the report what i am what i am with me now i am done. Just go with it now. Next week will take. Will take care more than anything to one thing since today we already did bharat part we can do you can do some edits as you are saying joshua and its if it's possible we can record tomorrow is that ok. Jada finding great ok guys then let's meet tomorrow. Alright x. My boy.Everybody good to have you all back. Saturday the agenda for a meeting is discussing about the pipeline and how we are going to go ahead with our chosen data set. To ask for a previous call we decided the new york cab data set. And i said to go with the flow. What you say guys are you all prep da video parts like do you have a plan ready. I have done with cocoa powder. Mowgli shahrukh khan to the standard but i am the best. Bharat antony. Yeah i have done the top extraction of the data set from the official website as well as the transformation path lekin setting path. And what about you since i've discovered too much into the basic part so i am comfortable with different introduction because i think later on and used there you will you guys will later on to the remaining parts right because. Telling me she is done something about the recent. Ok. Sudden talking about my part guys i have done some detailed study on the exploratory data analysis. So yaar using various visualisations and other libraries. Core data analysis i have prepared something. So yeah we can i can go ahead with it. Ok. And just to ask you guys. We are starting with v we have to cover everything in life 15 minutes right so are we taking down at time alike in how much minute every one of us is going to speak. No not like each of good now. 15 minutes. No no the overall time is 15 minutes so how to how to divide time amongst us how to divide 15 minutes. Tamil think first will start on portion and winters we have got anything related to time. Ok below as i was looking at you can use the same frequency very presenting. We took four to five minutes. So you can have the same sort of explanation why why i'm asking this because we have 15 minutes and letters not in home over outreach this time limit and since my portion is only for the introduction of the technical part so i can cover it up and let sit 2 to 3 minutes so that you guys can get more time. Fine for now but now we are trying to present the can i sell so you don't have to. Ok i am let's now before starting my introduction should we give a little bit let's discuss about the whole overview of the project so that. We are clear with what is the flow of the whole project. After introduction what part is going to cover up in all. Yes hua hai. Bharat ke explain something about this. I can do actually one thing that let you know when we no speak it as a group let's make this for example it can be is the person like you know if she is speaking the introduction part like a. You if any if anyone wants to question you like no for your path. Lift it like that let you know in a lot like a random thing like you know. First you will have some question and then i'll ask if it overlaps not definitely like after the transformation and when we do that nlp part it so. Irritating allah kamini cleaning process request. To think about those things. What do you say i like the question then i will then i will process then the joshua so will fall this order. Would you say. Yeah yeah it makes perfect office ok so i think in my introduction part the whole data flow which we are going like i mean data pipeline details is covered in the abstract so i will cover that so i will start with my introduction part is it ok guys. Movies tv on this rate. Yeah. Alright so as we choose the data source that is a new york taxi why because new york taxi is i mean in the new york taxi cab it's one of the very famous transportation which is used not only in new york buttonholes united states of america because it's one of the most busiest form. Most busiest taxi commute. And the these people what they have done they have made the data source quite public so that their facilities can be taken care of their facilities can be taken care and can be maintained in a proper way so what they have done is tlc that is new york taxi. And limousine commission has made their private data public. In order to make some observations that data scientist who are who can use this data set. To make some observation about the pickup and drop not only pickup and drop but who can make more inferences and predictions out of this data set so this is one of the most trending part of the new york taxi data path. Then later on coming to what exactly this data set consists of it has all the details about the geographical location like latitude longitude the arrival time time departure time mostly at what time of the day the other travels are very frequent and which part of the day the travels are less frequent then also about the details about the fair and tips charges total distance that is traveled also which part of the new york. New york city. Sorry to cut you down yeah. I think we should. Start recording this food waste a lot of time again are. Taking all of fast let's let's take the random speaking that put the recorder on speak from the beginning will record what's a. So we are also using this data set we can understand that which part of the new york city which locality is used to travel more which locality is used by people to travel more in in in a day. And this was our motivation why did we choose this data set. And coming ok so giving you more information about this data set is that actually new york taxi cab had initiative of cities city street smart technique. And also it was estimated that around 20 million trips have been calculated per month which is done by new york tak capsi sorry new york taxi cab. I don't to this this all will help us to get some insights from this data set. Now giving about the whole overview of the data pipeline project which we are going to do so we're trying to evaluate the data streaming process in a message system and perform data transformation on the data set that leads to contemporary observation based on the data visualisation. And a big data visualisation how it is going to how it is going to be done his buy used by using data visualisation we are trying to figure out what exactly the average fair passenger pay in total distance that they have covered and also what is the strength of the passenger that i mean if a passengers travelling alone or they are with their own family so we are trying to do this and the whole pipeline process comprises of three steps one is data in session data transformation and then data storage now i give a little bit overview of whole project details which we have done will start from we got the data of tlc new york taxi and then using python programming language we we have sent it in the kafka producer to multiple brokers which gives us better reliability fault tolerance and perfection and along with that we have used many producer and consumer configuration which will help us to achieve many factors like high durability high throughput sand complete transfer process will be very efficient and secure and in this way and then we tried to store data in mongodb db with the no sql database and this will try this will be tried this will try to search and filter data from the. Mango tv with an analysis process and after that we are making some predictions using machine learning. And different different algorithms now there were also some challenges which we faced during the whole time in which can be which we will be facing accord into me i am just trying to protect. When is that in solution of kafka is tiresome because it can take time and the broker can feel because it has long directories. Then also to create a new log directory and to save the server properties in a file that can be a challenge so. This is what exactly can be challenges in the this is whole process now a little bit explaining about the details of information that is in this new york taxi cab. We have column names like vendor id then trade code pickup longitude latitude then passenger account trip distance extra. This is this was just to give you some little bit details about what what columns are maintained in this data set. So i think this is it for the introduction part. And. Nothing much to be done more. Thank you can be i think that was a very detailed introduction you given to order fast. Appreciate your effort into this. Thank you. Agar is any more questions on this. Not at the moment if there are any and every. This is short so let's record was austria's part next because. We have to submit this we have a deadline right. So blood is now decide who is going to present at your home. I think our next should be bharat spark writing he took the initiative for lessons. Yes correct correct to that. Address everyone. Bharat are you there in the meeting. Yah. So a let's give him a day of one or two just to prepare and then we can catch up again to his. Room. For you. Why. Ok you have done ready. So i am preparing the instruction please serve. Transfer process. Do i need. I need. At least one week minimum 2 lakh no prepare for. Ok so next time ok fine so we can meet up next week and then bharat you can show us your work and then we will continue with other people also. Sure. Ok keypad. And if there are in meanwhile if you have any changes in the introduction part please let me know. Sure i'll let you know. Ok guys. Ok baba ok bye bye."""
# Tokenizing the text 
stopWords = set(stopwords.words("english")) 
words = word_tokenize(text) 
   
# Creating a frequency table to keep the  
# score of each word 
   
freqTable = dict() 
for word in words: 
    word = word.lower() 
    if word in stopWords: 
        continue
    if word in freqTable: 
        freqTable[word] += 1
    else: 
        freqTable[word] = 1
   
# Creating a dictionary to keep the score 
# of each sentence 
sentences = sent_tokenize(text) 
sentenceValue = dict() 
   
for sentence in sentences: 
    for word, freq in freqTable.items(): 
        if word in sentence.lower(): 
            if sentence in sentenceValue: 
                sentenceValue[sentence] += freq 
            else: 
                sentenceValue[sentence] = freq 
   
   
   
sumValues = 0
for sentence in sentenceValue: 
    sumValues += sentenceValue[sentence] 
   
# Average value of a sentence from the original text 
   
average = int(sumValues / len(sentenceValue)) 
   
# Storing sentences into our summary. 
summary = '' 
for sentence in sentences: 
    if (sentence in sentenceValue) and (sentenceValue[sentence] > (1.2 * average)): 
        summary += " " + sentence 
print(summary) 


# In[ ]:




